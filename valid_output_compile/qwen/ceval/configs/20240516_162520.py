ceval_datasets=[
    dict(abbr='ceval-computer_network',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于计算机网络考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='computer_network',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-operating_system',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于操作系统考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='operating_system',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-computer_architecture',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于计算机组成考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='computer_architecture',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-college_programming',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于大学编程考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_programming',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-college_physics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于大学物理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_physics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-college_chemistry',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于大学化学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_chemistry',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-advanced_mathematics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于高等数学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='advanced_mathematics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-probability_and_statistics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于概率统计考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='probability_and_statistics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-discrete_mathematics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于离散数学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='discrete_mathematics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-electrical_engineer',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于注册电气工程师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='electrical_engineer',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-metrology_engineer',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于注册计量师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='metrology_engineer',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-high_school_mathematics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于高中数学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_mathematics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-high_school_physics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于高中物理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_physics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-high_school_chemistry',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于高中化学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_chemistry',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-high_school_biology',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于高中生物考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_biology',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-middle_school_mathematics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于初中数学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_mathematics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-middle_school_biology',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于初中生物考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_biology',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-middle_school_physics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于初中物理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_physics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-middle_school_chemistry',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于初中化学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_chemistry',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-veterinary_medicine',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于兽医学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='veterinary_medicine',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-college_economics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于大学经济学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_economics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-business_administration',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于工商管理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='business_administration',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-marxism',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于马克思主义基本原理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='marxism',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-mao_zedong_thought',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于毛泽东思想和中国特色社会主义理论体系概论考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='mao_zedong_thought',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-education_science',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于教育学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='education_science',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-teacher_qualification',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于教师资格考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='teacher_qualification',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-high_school_politics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于高中政治考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_politics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-high_school_geography',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于高中地理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_geography',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-middle_school_politics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于初中政治考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_politics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-middle_school_geography',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于初中地理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_geography',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-modern_chinese_history',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于近代史纲要考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='modern_chinese_history',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-ideological_and_moral_cultivation',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于思想道德修养与法律基础考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='ideological_and_moral_cultivation',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-logic',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于逻辑学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='logic',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-law',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于法学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='law',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-chinese_language_and_literature',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于中国语言文学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='chinese_language_and_literature',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-art_studies',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于艺术学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='art_studies',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-professional_tour_guide',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于导游资格考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='professional_tour_guide',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-legal_professional',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于法律职业资格考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='legal_professional',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-high_school_chinese',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于高中语文考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_chinese',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-high_school_history',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于高中历史考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_history',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-middle_school_history',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于初中历史考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_history',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-civil_servant',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于公务员考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='civil_servant',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-sports_science',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于体育学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='sports_science',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-plant_protection',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于植物保护考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='plant_protection',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-basic_medicine',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于基础医学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='basic_medicine',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-clinical_medicine',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于临床医学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='clinical_medicine',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-urban_and_rural_planner',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于注册城乡规划师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='urban_and_rural_planner',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-accountant',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于注册会计师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='accountant',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-fire_engineer',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于注册消防工程师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='fire_engineer',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-environmental_impact_assessment_engineer',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于环境影响评价工程师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='environmental_impact_assessment_engineer',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-tax_accountant',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于税务师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='tax_accountant',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-physician',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于医师资格考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='physician',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    ]
datasets=[
    dict(abbr='ceval-computer_network',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于计算机网络考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='computer_network',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-operating_system',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于操作系统考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='operating_system',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-computer_architecture',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于计算机组成考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='computer_architecture',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-college_programming',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于大学编程考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_programming',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-college_physics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于大学物理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_physics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-college_chemistry',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于大学化学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_chemistry',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-advanced_mathematics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于高等数学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='advanced_mathematics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-probability_and_statistics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于概率统计考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='probability_and_statistics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-discrete_mathematics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于离散数学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='discrete_mathematics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-electrical_engineer',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于注册电气工程师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='electrical_engineer',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-metrology_engineer',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于注册计量师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='metrology_engineer',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-high_school_mathematics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于高中数学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_mathematics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-high_school_physics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于高中物理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_physics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-high_school_chemistry',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于高中化学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_chemistry',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-high_school_biology',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于高中生物考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_biology',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-middle_school_mathematics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于初中数学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_mathematics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-middle_school_biology',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于初中生物考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_biology',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-middle_school_physics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于初中物理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_physics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-middle_school_chemistry',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于初中化学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_chemistry',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-veterinary_medicine',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于兽医学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='veterinary_medicine',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-college_economics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于大学经济学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_economics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-business_administration',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于工商管理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='business_administration',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-marxism',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于马克思主义基本原理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='marxism',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-mao_zedong_thought',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于毛泽东思想和中国特色社会主义理论体系概论考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='mao_zedong_thought',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-education_science',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于教育学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='education_science',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-teacher_qualification',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于教师资格考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='teacher_qualification',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-high_school_politics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于高中政治考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_politics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-high_school_geography',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于高中地理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_geography',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-middle_school_politics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于初中政治考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_politics',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-middle_school_geography',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于初中地理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_geography',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-modern_chinese_history',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于近代史纲要考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='modern_chinese_history',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-ideological_and_moral_cultivation',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于思想道德修养与法律基础考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='ideological_and_moral_cultivation',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-logic',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于逻辑学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='logic',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-law',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于法学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='law',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-chinese_language_and_literature',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于中国语言文学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='chinese_language_and_literature',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-art_studies',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于艺术学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='art_studies',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-professional_tour_guide',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于导游资格考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='professional_tour_guide',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-legal_professional',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于法律职业资格考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='legal_professional',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-high_school_chinese',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于高中语文考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_chinese',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-high_school_history',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于高中历史考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_history',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-middle_school_history',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于初中历史考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_history',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-civil_servant',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于公务员考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='civil_servant',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-sports_science',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于体育学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='sports_science',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-plant_protection',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于植物保护考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='plant_protection',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-basic_medicine',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于基础医学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='basic_medicine',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-clinical_medicine',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于临床医学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='clinical_medicine',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-urban_and_rural_planner',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于注册城乡规划师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='urban_and_rural_planner',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-accountant',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于注册会计师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='accountant',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-fire_engineer',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于注册消防工程师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='fire_engineer',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-environmental_impact_assessment_engineer',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于环境影响评价工程师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='environmental_impact_assessment_engineer',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-tax_accountant',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于税务师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='tax_accountant',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(abbr='ceval-physician',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.utils.text_postprocessors.first_capital_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='以下是中国关于医师资格考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='physician',
        path='./data/ceval/formal_ceval',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='answer',
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    ]
gsm8k_datasets=[
    dict(abbr='gsm8k',
        eval_cfg=dict(
            dataset_postprocessor=dict(
                type='opencompass.datasets.gsm8k_dataset_postprocess'),
            evaluator=dict(
                type='opencompass.datasets.Gsm8kEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.datasets.gsm8k_postprocess')),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=512,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt="Question: Angelo and Melanie want to plan how many hours over the next week they should study together for their test next week. They have 2 chapters of their textbook to study and 4 worksheets to memorize. They figure out that they should dedicate 3 hours to each chapter of their textbook and 1.5 hours for each worksheet. If they plan to study no more than 4 hours each day, how many days should they plan to study total over the next week if they take a 10-minute break every hour, include 3 10-minute snack breaks each day, and 30 minutes for lunch each day?\nLet's think step by step\nAnswer:",
                            role='HUMAN'),
                        dict(prompt='Angelo and Melanie think they should dedicate 3 hours to each of the 2 chapters, 3 hours x 2 chapters = 6 hours total.\nFor the worksheets they plan to dedicate 1.5 hours for each worksheet, 1.5 hours x 4 worksheets = 6 hours total.\nAngelo and Melanie need to start with planning 12 hours to study, at 4 hours a day, 12 / 4 = 3 days.\nHowever, they need to include time for breaks and lunch. Every hour they want to include a 10-minute break, so 12 total hours x 10 minutes = 120 extra minutes for breaks.\nThey also want to include 3 10-minute snack breaks, 3 x 10 minutes = 30 minutes.\nAnd they want to include 30 minutes for lunch each day, so 120 minutes for breaks + 30 minutes for snack breaks + 30 minutes for lunch = 180 minutes, or 180 / 60 minutes per hour = 3 extra hours.\nSo Angelo and Melanie want to plan 12 hours to study + 3 hours of breaks = 15 hours total.\nThey want to study no more than 4 hours each day, 15 hours / 4 hours each day = 3.75\nThey will need to plan to study 4 days to allow for all the time they need.\nThe answer is 4\n',
                            role='BOT'),
                        dict(prompt="Question: Mark's basketball team scores 25 2 pointers, 8 3 pointers and 10 free throws.  Their opponents score double the 2 pointers but half the 3 pointers and free throws.  What's the total number of points scored by both teams added together?\nLet's think step by step\nAnswer:",
                            role='HUMAN'),
                        dict(prompt="Mark's team scores 25 2 pointers, meaning they scored 25*2= 50 points in 2 pointers.\nHis team also scores 6 3 pointers, meaning they scored 8*3= 24 points in 3 pointers\nThey scored 10 free throws, and free throws count as one point so they scored 10*1=10 points in free throws.\nAll together his team scored 50+24+10= 84 points\nMark's opponents scored double his team's number of 2 pointers, meaning they scored 50*2=100 points in 2 pointers.\nHis opponents scored half his team's number of 3 pointers, meaning they scored 24/2= 12 points in 3 pointers.\nThey also scored half Mark's team's points in free throws, meaning they scored 10/2=5 points in free throws.\nAll together Mark's opponents scored 100+12+5=117 points\nThe total score for the game is both team's scores added together, so it is 84+117=201 points\nThe answer is 201\n",
                            role='BOT'),
                        dict(prompt="Question: Bella has two times as many marbles as frisbees. She also has 20 more frisbees than deck cards. If she buys 2/5 times more of each item, what would be the total number of the items she will have if she currently has 60 marbles?\nLet's think step by step\nAnswer:",
                            role='HUMAN'),
                        dict(prompt="When Bella buys 2/5 times more marbles, she'll have increased the number of marbles by 2/5*60 = 24\nThe total number of marbles she'll have is 60+24 = 84\nIf Bella currently has 60 marbles, and she has two times as many marbles as frisbees, she has 60/2 = 30 frisbees.\nIf Bella buys 2/5 times more frisbees, she'll have 2/5*30 = 12 more frisbees.\nThe total number of frisbees she'll have will increase to 30+12 = 42\nBella also has 20 more frisbees than deck cards, meaning she has 30-20 = 10 deck cards\nIf she buys 2/5 times more deck cards, she'll have 2/5*10 = 4 more deck cards.\nThe total number of deck cards she'll have is 10+4 = 14\nTogether, Bella will have a total of 14+42+84 = 140 items\nThe answer is 140\n",
                            role='BOT'),
                        dict(prompt="Question: A group of 4 fruit baskets contains 9 apples, 15 oranges, and 14 bananas in the first three baskets and 2 less of each fruit in the fourth basket. How many fruits are there?\nLet's think step by step\nAnswer:",
                            role='HUMAN'),
                        dict(prompt='For the first three baskets, the number of apples and oranges in one basket is 9+15=24\nIn total, together with bananas, the number of fruits in one basket is 24+14=38 for the first three baskets.\nSince there are three baskets each having 38 fruits, there are 3*38=114 fruits in the first three baskets.\nThe number of apples in the fourth basket is 9-2=7\nThere are also 15-2=13 oranges in the fourth basket\nThe combined number of oranges and apples in the fourth basket is 13+7=20\nThe fourth basket also contains 14-2=12 bananas.\nIn total, the fourth basket has 20+12=32 fruits.\nThe four baskets together have 32+114=146 fruits.\nThe answer is 146\n',
                            role='BOT'),
                        dict(prompt="Question: {question}\nLet's think step by step\nAnswer:",
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/gsm8k',
        reader_cfg=dict(
            input_columns=[
                'question',
                ],
            output_column='answer'),
        type='opencompass.datasets.GSM8KDataset'),
    ]
humaneval_datasets=[
    dict(abbr='openai_humaneval',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.datasets.HumanEvaluator'),
            k=[
                1,
                10,
                100,
                ],
            pred_postprocessor=dict(
                type='opencompass.datasets.humaneval_postprocess_v2'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=512,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='Complete the following python code:\n{prompt}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='./data/humaneval/human-eval-v2-20210705.jsonl',
        reader_cfg=dict(
            input_columns=[
                'prompt',
                ],
            output_column='task_id',
            train_split='test'),
        type='opencompass.datasets.HumanevalDataset'),
    ]
infer=dict(
    partitioner=dict(
        type='opencompass.partitioners.NaivePartitioner'),
    runner=dict(
        concurrent_users=1,
        max_num_workers=4,
        task=dict(
            type='opencompass.tasks.OpenICLInferTask'),
        type='opencompass.runners.local_api.LocalAPIRunner'))
mmlu_datasets=[
    dict(abbr='lukaemon_mmlu_college_biology',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about college biology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about college biology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_biology',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_college_chemistry',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about college chemistry. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about college chemistry. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_chemistry',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_college_computer_science',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about college computer science. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about college computer science. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_computer_science',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_college_mathematics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about college mathematics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about college mathematics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_mathematics',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_college_physics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about college physics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about college physics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_physics',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_electrical_engineering',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about electrical engineering. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about electrical engineering. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='electrical_engineering',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_astronomy',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about astronomy. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about astronomy. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='astronomy',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_anatomy',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about anatomy. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about anatomy. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='anatomy',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_abstract_algebra',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about abstract algebra. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about abstract algebra. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='abstract_algebra',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_machine_learning',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about machine learning. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about machine learning. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='machine_learning',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_clinical_knowledge',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about clinical knowledge. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about clinical knowledge. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='clinical_knowledge',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_global_facts',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about global facts. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about global facts. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='global_facts',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_management',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about management. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about management. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='management',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_nutrition',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about nutrition. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about nutrition. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='nutrition',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_marketing',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about marketing. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about marketing. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='marketing',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_professional_accounting',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about professional accounting. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about professional accounting. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='professional_accounting',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_high_school_geography',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about high school geography. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about high school geography. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_geography',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_international_law',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about international law. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about international law. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='international_law',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_moral_scenarios',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about moral scenarios. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about moral scenarios. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='moral_scenarios',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_computer_security',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about computer security. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about computer security. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='computer_security',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_high_school_microeconomics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about high school microeconomics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about high school microeconomics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_microeconomics',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_professional_law',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about professional law. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about professional law. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='professional_law',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_medical_genetics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about medical genetics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about medical genetics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='medical_genetics',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_professional_psychology',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about professional psychology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about professional psychology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='professional_psychology',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_jurisprudence',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about jurisprudence. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about jurisprudence. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='jurisprudence',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_world_religions',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about world religions. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about world religions. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='world_religions',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_philosophy',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about philosophy. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about philosophy. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='philosophy',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_virology',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about virology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about virology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='virology',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_high_school_chemistry',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about high school chemistry. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about high school chemistry. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_chemistry',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_public_relations',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about public relations. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about public relations. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='public_relations',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_high_school_macroeconomics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about high school macroeconomics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about high school macroeconomics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_macroeconomics',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_human_sexuality',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about human sexuality. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about human sexuality. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='human_sexuality',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_elementary_mathematics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about elementary mathematics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about elementary mathematics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='elementary_mathematics',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_high_school_physics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about high school physics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about high school physics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_physics',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_high_school_computer_science',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about high school computer science. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about high school computer science. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_computer_science',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_high_school_european_history',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about high school european history. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about high school european history. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_european_history',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_business_ethics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about business ethics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about business ethics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='business_ethics',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_moral_disputes',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about moral disputes. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about moral disputes. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='moral_disputes',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_high_school_statistics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about high school statistics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about high school statistics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_statistics',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_miscellaneous',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about miscellaneous. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about miscellaneous. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='miscellaneous',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_formal_logic',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about formal logic. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about formal logic. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='formal_logic',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_high_school_government_and_politics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about high school government and politics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about high school government and politics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_government_and_politics',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_prehistory',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about prehistory. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about prehistory. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='prehistory',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_security_studies',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about security studies. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about security studies. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='security_studies',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_high_school_biology',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about high school biology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about high school biology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_biology',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_logical_fallacies',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about logical fallacies. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about logical fallacies. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='logical_fallacies',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_high_school_world_history',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about high school world history. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about high school world history. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_world_history',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_professional_medicine',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about professional medicine. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about professional medicine. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='professional_medicine',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_high_school_mathematics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about high school mathematics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about high school mathematics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_mathematics',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_college_medicine',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about college medicine. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about college medicine. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_medicine',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_high_school_us_history',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about high school us history. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about high school us history. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_us_history',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_sociology',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about sociology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about sociology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='sociology',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_econometrics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about econometrics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about econometrics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='econometrics',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_high_school_psychology',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about high school psychology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about high school psychology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_psychology',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_human_aging',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about human aging. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about human aging. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='human_aging',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_us_foreign_policy',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about us foreign policy. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about us foreign policy. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='us_foreign_policy',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(abbr='lukaemon_mmlu_conceptual_physics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type='opencompass.utils.text_postprocessors.first_option_postprocess')),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(
                    round=[
                        dict(prompt='There is a single choice question about conceptual physics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        dict(prompt='{target}\n',
                            role='BOT'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(prompt='There is a single choice question about conceptual physics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                    ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='conceptual_physics',
        path='./data/mmlu/',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
                ],
            output_column='target',
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    ]
models=[
    dict(abbr='qwen-14b-chat',
        batch_size=2,
        generation_kwargs=dict(
            enable_search=False),
        max_out_len=2048,
        max_seq_len=4096,
        path='Qwen',
        query_per_second=1,
        requesturl='https://acv-gydn.baocloud.cn/acv-service/service/api/6000002067298078/v1/predict/openaiapi/v1/chat/completions',
        temperature=0.1,
        top_p=0,
        type='opencompass.models.Qwen_local'),
    ]
summarizer=dict(
    dataset_abbrs=[
        '--------- 考试 Exam ---------',
        'ceval',
        'agieval',
        'mmlu',
        'GaokaoBench',
        'ARC-c',
        '--------- 语言 Language ---------',
        'WiC',
        'summedits',
        'chid-dev',
        'afqmc-dev',
        'bustm-dev',
        'cluewsc-dev',
        'WSC',
        'winogrande',
        'flores_100',
        '--------- 知识 Knowledge ---------',
        'BoolQ',
        'commonsense_qa',
        'nq',
        'triviaqa',
        '--------- 推理 Reasoning ---------',
        'cmnli',
        'ocnli',
        'ocnli_fc-dev',
        'AX_b',
        'AX_g',
        'CB',
        'RTE',
        'story_cloze',
        'COPA',
        'ReCoRD',
        'hellaswag',
        'piqa',
        'siqa',
        'strategyqa',
        'math',
        'gsm8k',
        'TheoremQA',
        'openai_humaneval',
        'mbpp',
        'bbh',
        '--------- 理解 Understanding ---------',
        'C3',
        'CMRC_dev',
        'DRCD_dev',
        'MultiRC',
        'race-middle',
        'race-high',
        'openbookqa_fact',
        'csl_dev',
        'lcsts',
        'Xsum',
        'eprstmt-dev',
        'lambada',
        'tnews-dev',
        ],
    summary_groups=[
        dict(name='agieval-chinese',
            subsets=[
                'agieval-gaokao-chinese',
                'agieval-gaokao-english',
                'agieval-gaokao-geography',
                'agieval-gaokao-history',
                'agieval-gaokao-biology',
                'agieval-gaokao-chemistry',
                'agieval-gaokao-physics',
                'agieval-gaokao-mathqa',
                'agieval-logiqa-zh',
                'agieval-jec-qa-kd',
                'agieval-jec-qa-ca',
                'agieval-gaokao-mathcloze',
                ]),
        dict(name='agieval-english',
            subsets=[
                'agieval-lsat-ar',
                'agieval-lsat-lr',
                'agieval-lsat-rc',
                'agieval-logiqa-en',
                'agieval-sat-math',
                'agieval-sat-en',
                'agieval-sat-en-without-passage',
                'agieval-aqua-rat',
                'agieval-math',
                ]),
        dict(name='agieval-gaokao',
            subsets=[
                'agieval-gaokao-chinese',
                'agieval-gaokao-english',
                'agieval-gaokao-geography',
                'agieval-gaokao-history',
                'agieval-gaokao-biology',
                'agieval-gaokao-chemistry',
                'agieval-gaokao-physics',
                'agieval-gaokao-mathqa',
                'agieval-gaokao-mathcloze',
                ]),
        dict(name='agieval',
            subsets=[
                'agieval-gaokao-chinese',
                'agieval-gaokao-english',
                'agieval-gaokao-geography',
                'agieval-gaokao-history',
                'agieval-gaokao-biology',
                'agieval-gaokao-chemistry',
                'agieval-gaokao-physics',
                'agieval-gaokao-mathqa',
                'agieval-logiqa-zh',
                'agieval-lsat-ar',
                'agieval-lsat-lr',
                'agieval-lsat-rc',
                'agieval-logiqa-en',
                'agieval-sat-math',
                'agieval-sat-en',
                'agieval-sat-en-without-passage',
                'agieval-aqua-rat',
                'agieval-jec-qa-kd',
                'agieval-jec-qa-ca',
                'agieval-gaokao-mathcloze',
                'agieval-math',
                ]),
        dict(name='mmlu-humanities',
            subsets=[
                'lukaemon_mmlu_formal_logic',
                'lukaemon_mmlu_high_school_european_history',
                'lukaemon_mmlu_high_school_us_history',
                'lukaemon_mmlu_high_school_world_history',
                'lukaemon_mmlu_international_law',
                'lukaemon_mmlu_jurisprudence',
                'lukaemon_mmlu_logical_fallacies',
                'lukaemon_mmlu_moral_disputes',
                'lukaemon_mmlu_moral_scenarios',
                'lukaemon_mmlu_philosophy',
                'lukaemon_mmlu_prehistory',
                'lukaemon_mmlu_professional_law',
                'lukaemon_mmlu_world_religions',
                ]),
        dict(name='mmlu-stem',
            subsets=[
                'lukaemon_mmlu_abstract_algebra',
                'lukaemon_mmlu_anatomy',
                'lukaemon_mmlu_astronomy',
                'lukaemon_mmlu_college_biology',
                'lukaemon_mmlu_college_chemistry',
                'lukaemon_mmlu_college_computer_science',
                'lukaemon_mmlu_college_mathematics',
                'lukaemon_mmlu_college_physics',
                'lukaemon_mmlu_computer_security',
                'lukaemon_mmlu_conceptual_physics',
                'lukaemon_mmlu_electrical_engineering',
                'lukaemon_mmlu_elementary_mathematics',
                'lukaemon_mmlu_high_school_biology',
                'lukaemon_mmlu_high_school_chemistry',
                'lukaemon_mmlu_high_school_computer_science',
                'lukaemon_mmlu_high_school_mathematics',
                'lukaemon_mmlu_high_school_physics',
                'lukaemon_mmlu_high_school_statistics',
                'lukaemon_mmlu_machine_learning',
                ]),
        dict(name='mmlu-social-science',
            subsets=[
                'lukaemon_mmlu_econometrics',
                'lukaemon_mmlu_high_school_geography',
                'lukaemon_mmlu_high_school_government_and_politics',
                'lukaemon_mmlu_high_school_macroeconomics',
                'lukaemon_mmlu_high_school_microeconomics',
                'lukaemon_mmlu_high_school_psychology',
                'lukaemon_mmlu_human_sexuality',
                'lukaemon_mmlu_professional_psychology',
                'lukaemon_mmlu_public_relations',
                'lukaemon_mmlu_security_studies',
                'lukaemon_mmlu_sociology',
                'lukaemon_mmlu_us_foreign_policy',
                ]),
        dict(name='mmlu-other',
            subsets=[
                'lukaemon_mmlu_business_ethics',
                'lukaemon_mmlu_clinical_knowledge',
                'lukaemon_mmlu_college_medicine',
                'lukaemon_mmlu_global_facts',
                'lukaemon_mmlu_human_aging',
                'lukaemon_mmlu_management',
                'lukaemon_mmlu_marketing',
                'lukaemon_mmlu_medical_genetics',
                'lukaemon_mmlu_miscellaneous',
                'lukaemon_mmlu_nutrition',
                'lukaemon_mmlu_professional_accounting',
                'lukaemon_mmlu_professional_medicine',
                'lukaemon_mmlu_virology',
                ]),
        dict(name='mmlu',
            subsets=[
                'lukaemon_mmlu_formal_logic',
                'lukaemon_mmlu_high_school_european_history',
                'lukaemon_mmlu_high_school_us_history',
                'lukaemon_mmlu_high_school_world_history',
                'lukaemon_mmlu_international_law',
                'lukaemon_mmlu_jurisprudence',
                'lukaemon_mmlu_logical_fallacies',
                'lukaemon_mmlu_moral_disputes',
                'lukaemon_mmlu_moral_scenarios',
                'lukaemon_mmlu_philosophy',
                'lukaemon_mmlu_prehistory',
                'lukaemon_mmlu_professional_law',
                'lukaemon_mmlu_world_religions',
                'lukaemon_mmlu_abstract_algebra',
                'lukaemon_mmlu_anatomy',
                'lukaemon_mmlu_astronomy',
                'lukaemon_mmlu_college_biology',
                'lukaemon_mmlu_college_chemistry',
                'lukaemon_mmlu_college_computer_science',
                'lukaemon_mmlu_college_mathematics',
                'lukaemon_mmlu_college_physics',
                'lukaemon_mmlu_computer_security',
                'lukaemon_mmlu_conceptual_physics',
                'lukaemon_mmlu_electrical_engineering',
                'lukaemon_mmlu_elementary_mathematics',
                'lukaemon_mmlu_high_school_biology',
                'lukaemon_mmlu_high_school_chemistry',
                'lukaemon_mmlu_high_school_computer_science',
                'lukaemon_mmlu_high_school_mathematics',
                'lukaemon_mmlu_high_school_physics',
                'lukaemon_mmlu_high_school_statistics',
                'lukaemon_mmlu_machine_learning',
                'lukaemon_mmlu_econometrics',
                'lukaemon_mmlu_high_school_geography',
                'lukaemon_mmlu_high_school_government_and_politics',
                'lukaemon_mmlu_high_school_macroeconomics',
                'lukaemon_mmlu_high_school_microeconomics',
                'lukaemon_mmlu_high_school_psychology',
                'lukaemon_mmlu_human_sexuality',
                'lukaemon_mmlu_professional_psychology',
                'lukaemon_mmlu_public_relations',
                'lukaemon_mmlu_security_studies',
                'lukaemon_mmlu_sociology',
                'lukaemon_mmlu_us_foreign_policy',
                'lukaemon_mmlu_business_ethics',
                'lukaemon_mmlu_clinical_knowledge',
                'lukaemon_mmlu_college_medicine',
                'lukaemon_mmlu_global_facts',
                'lukaemon_mmlu_human_aging',
                'lukaemon_mmlu_management',
                'lukaemon_mmlu_marketing',
                'lukaemon_mmlu_medical_genetics',
                'lukaemon_mmlu_miscellaneous',
                'lukaemon_mmlu_nutrition',
                'lukaemon_mmlu_professional_accounting',
                'lukaemon_mmlu_professional_medicine',
                'lukaemon_mmlu_virology',
                ]),
        dict(name='mmlu-weighted',
            subsets=[
                'lukaemon_mmlu_formal_logic',
                'lukaemon_mmlu_high_school_european_history',
                'lukaemon_mmlu_high_school_us_history',
                'lukaemon_mmlu_high_school_world_history',
                'lukaemon_mmlu_international_law',
                'lukaemon_mmlu_jurisprudence',
                'lukaemon_mmlu_logical_fallacies',
                'lukaemon_mmlu_moral_disputes',
                'lukaemon_mmlu_moral_scenarios',
                'lukaemon_mmlu_philosophy',
                'lukaemon_mmlu_prehistory',
                'lukaemon_mmlu_professional_law',
                'lukaemon_mmlu_world_religions',
                'lukaemon_mmlu_abstract_algebra',
                'lukaemon_mmlu_anatomy',
                'lukaemon_mmlu_astronomy',
                'lukaemon_mmlu_college_biology',
                'lukaemon_mmlu_college_chemistry',
                'lukaemon_mmlu_college_computer_science',
                'lukaemon_mmlu_college_mathematics',
                'lukaemon_mmlu_college_physics',
                'lukaemon_mmlu_computer_security',
                'lukaemon_mmlu_conceptual_physics',
                'lukaemon_mmlu_electrical_engineering',
                'lukaemon_mmlu_elementary_mathematics',
                'lukaemon_mmlu_high_school_biology',
                'lukaemon_mmlu_high_school_chemistry',
                'lukaemon_mmlu_high_school_computer_science',
                'lukaemon_mmlu_high_school_mathematics',
                'lukaemon_mmlu_high_school_physics',
                'lukaemon_mmlu_high_school_statistics',
                'lukaemon_mmlu_machine_learning',
                'lukaemon_mmlu_econometrics',
                'lukaemon_mmlu_high_school_geography',
                'lukaemon_mmlu_high_school_government_and_politics',
                'lukaemon_mmlu_high_school_macroeconomics',
                'lukaemon_mmlu_high_school_microeconomics',
                'lukaemon_mmlu_high_school_psychology',
                'lukaemon_mmlu_human_sexuality',
                'lukaemon_mmlu_professional_psychology',
                'lukaemon_mmlu_public_relations',
                'lukaemon_mmlu_security_studies',
                'lukaemon_mmlu_sociology',
                'lukaemon_mmlu_us_foreign_policy',
                'lukaemon_mmlu_business_ethics',
                'lukaemon_mmlu_clinical_knowledge',
                'lukaemon_mmlu_college_medicine',
                'lukaemon_mmlu_global_facts',
                'lukaemon_mmlu_human_aging',
                'lukaemon_mmlu_management',
                'lukaemon_mmlu_marketing',
                'lukaemon_mmlu_medical_genetics',
                'lukaemon_mmlu_miscellaneous',
                'lukaemon_mmlu_nutrition',
                'lukaemon_mmlu_professional_accounting',
                'lukaemon_mmlu_professional_medicine',
                'lukaemon_mmlu_virology',
                ],
            weights=dict(
                lukaemon_mmlu_abstract_algebra=100,
                lukaemon_mmlu_anatomy=135,
                lukaemon_mmlu_astronomy=152,
                lukaemon_mmlu_business_ethics=100,
                lukaemon_mmlu_clinical_knowledge=265,
                lukaemon_mmlu_college_biology=144,
                lukaemon_mmlu_college_chemistry=100,
                lukaemon_mmlu_college_computer_science=100,
                lukaemon_mmlu_college_mathematics=100,
                lukaemon_mmlu_college_medicine=173,
                lukaemon_mmlu_college_physics=102,
                lukaemon_mmlu_computer_security=100,
                lukaemon_mmlu_conceptual_physics=235,
                lukaemon_mmlu_econometrics=114,
                lukaemon_mmlu_electrical_engineering=145,
                lukaemon_mmlu_elementary_mathematics=378,
                lukaemon_mmlu_formal_logic=126,
                lukaemon_mmlu_global_facts=100,
                lukaemon_mmlu_high_school_biology=310,
                lukaemon_mmlu_high_school_chemistry=203,
                lukaemon_mmlu_high_school_computer_science=100,
                lukaemon_mmlu_high_school_european_history=165,
                lukaemon_mmlu_high_school_geography=198,
                lukaemon_mmlu_high_school_government_and_politics=193,
                lukaemon_mmlu_high_school_macroeconomics=390,
                lukaemon_mmlu_high_school_mathematics=270,
                lukaemon_mmlu_high_school_microeconomics=238,
                lukaemon_mmlu_high_school_physics=151,
                lukaemon_mmlu_high_school_psychology=545,
                lukaemon_mmlu_high_school_statistics=216,
                lukaemon_mmlu_high_school_us_history=204,
                lukaemon_mmlu_high_school_world_history=237,
                lukaemon_mmlu_human_aging=223,
                lukaemon_mmlu_human_sexuality=131,
                lukaemon_mmlu_international_law=121,
                lukaemon_mmlu_jurisprudence=108,
                lukaemon_mmlu_logical_fallacies=163,
                lukaemon_mmlu_machine_learning=112,
                lukaemon_mmlu_management=103,
                lukaemon_mmlu_marketing=234,
                lukaemon_mmlu_medical_genetics=100,
                lukaemon_mmlu_miscellaneous=783,
                lukaemon_mmlu_moral_disputes=346,
                lukaemon_mmlu_moral_scenarios=895,
                lukaemon_mmlu_nutrition=306,
                lukaemon_mmlu_philosophy=311,
                lukaemon_mmlu_prehistory=324,
                lukaemon_mmlu_professional_accounting=282,
                lukaemon_mmlu_professional_law=1534,
                lukaemon_mmlu_professional_medicine=272,
                lukaemon_mmlu_professional_psychology=612,
                lukaemon_mmlu_public_relations=110,
                lukaemon_mmlu_security_studies=245,
                lukaemon_mmlu_sociology=201,
                lukaemon_mmlu_us_foreign_policy=100,
                lukaemon_mmlu_virology=166,
                lukaemon_mmlu_world_religions=171)),
        dict(name='cmmlu-humanities',
            subsets=[
                'cmmlu-arts',
                'cmmlu-chinese_history',
                'cmmlu-chinese_literature',
                'cmmlu-college_law',
                'cmmlu-global_facts',
                'cmmlu-international_law',
                'cmmlu-jurisprudence',
                'cmmlu-logical',
                'cmmlu-marxist_theory',
                'cmmlu-philosophy',
                'cmmlu-professional_law',
                'cmmlu-world_history',
                'cmmlu-world_religions',
                ]),
        dict(name='cmmlu-stem',
            subsets=[
                'cmmlu-anatomy',
                'cmmlu-astronomy',
                'cmmlu-college_actuarial_science',
                'cmmlu-college_engineering_hydrology',
                'cmmlu-college_mathematics',
                'cmmlu-college_medical_statistics',
                'cmmlu-computer_science',
                'cmmlu-conceptual_physics',
                'cmmlu-electrical_engineering',
                'cmmlu-elementary_mathematics',
                'cmmlu-genetics',
                'cmmlu-high_school_biology',
                'cmmlu-high_school_chemistry',
                'cmmlu-high_school_mathematics',
                'cmmlu-high_school_physics',
                'cmmlu-machine_learning',
                'cmmlu-virology',
                ]),
        dict(name='cmmlu-social-science',
            subsets=[
                'cmmlu-ancient_chinese',
                'cmmlu-business_ethics',
                'cmmlu-chinese_civil_service_exam',
                'cmmlu-chinese_food_culture',
                'cmmlu-chinese_foreign_policy',
                'cmmlu-chinese_teacher_qualification',
                'cmmlu-college_education',
                'cmmlu-economics',
                'cmmlu-education',
                'cmmlu-elementary_chinese',
                'cmmlu-ethnology',
                'cmmlu-high_school_geography',
                'cmmlu-high_school_politics',
                'cmmlu-journalism',
                'cmmlu-management',
                'cmmlu-marketing',
                'cmmlu-modern_chinese',
                'cmmlu-professional_accounting',
                'cmmlu-professional_psychology',
                'cmmlu-public_relations',
                'cmmlu-security_study',
                'cmmlu-sociology',
                ]),
        dict(name='cmmlu-other',
            subsets=[
                'cmmlu-agronomy',
                'cmmlu-chinese_driving_rule',
                'cmmlu-clinical_knowledge',
                'cmmlu-college_medicine',
                'cmmlu-computer_security',
                'cmmlu-construction_project_management',
                'cmmlu-elementary_commonsense',
                'cmmlu-elementary_information_and_technology',
                'cmmlu-food_science',
                'cmmlu-human_sexuality',
                'cmmlu-legal_and_moral_basis',
                'cmmlu-nutrition',
                'cmmlu-professional_medicine',
                'cmmlu-sports_science',
                'cmmlu-traditional_chinese_medicine',
                ]),
        dict(name='cmmlu-china-specific',
            subsets=[
                'cmmlu-ancient_chinese',
                'cmmlu-chinese_civil_service_exam',
                'cmmlu-chinese_driving_rule',
                'cmmlu-chinese_food_culture',
                'cmmlu-chinese_foreign_policy',
                'cmmlu-chinese_history',
                'cmmlu-chinese_literature',
                'cmmlu-chinese_teacher_qualification',
                'cmmlu-construction_project_management',
                'cmmlu-elementary_chinese',
                'cmmlu-elementary_commonsense',
                'cmmlu-ethnology',
                'cmmlu-high_school_politics',
                'cmmlu-modern_chinese',
                'cmmlu-traditional_chinese_medicine',
                ]),
        dict(name='cmmlu',
            subsets=[
                'cmmlu-agronomy',
                'cmmlu-anatomy',
                'cmmlu-ancient_chinese',
                'cmmlu-arts',
                'cmmlu-astronomy',
                'cmmlu-business_ethics',
                'cmmlu-chinese_civil_service_exam',
                'cmmlu-chinese_driving_rule',
                'cmmlu-chinese_food_culture',
                'cmmlu-chinese_foreign_policy',
                'cmmlu-chinese_history',
                'cmmlu-chinese_literature',
                'cmmlu-chinese_teacher_qualification',
                'cmmlu-college_actuarial_science',
                'cmmlu-college_education',
                'cmmlu-college_engineering_hydrology',
                'cmmlu-college_law',
                'cmmlu-college_mathematics',
                'cmmlu-college_medical_statistics',
                'cmmlu-clinical_knowledge',
                'cmmlu-college_medicine',
                'cmmlu-computer_science',
                'cmmlu-computer_security',
                'cmmlu-conceptual_physics',
                'cmmlu-construction_project_management',
                'cmmlu-economics',
                'cmmlu-education',
                'cmmlu-elementary_chinese',
                'cmmlu-elementary_commonsense',
                'cmmlu-elementary_information_and_technology',
                'cmmlu-electrical_engineering',
                'cmmlu-elementary_mathematics',
                'cmmlu-ethnology',
                'cmmlu-food_science',
                'cmmlu-genetics',
                'cmmlu-global_facts',
                'cmmlu-high_school_biology',
                'cmmlu-high_school_chemistry',
                'cmmlu-high_school_geography',
                'cmmlu-high_school_mathematics',
                'cmmlu-high_school_physics',
                'cmmlu-high_school_politics',
                'cmmlu-human_sexuality',
                'cmmlu-international_law',
                'cmmlu-journalism',
                'cmmlu-jurisprudence',
                'cmmlu-legal_and_moral_basis',
                'cmmlu-logical',
                'cmmlu-machine_learning',
                'cmmlu-management',
                'cmmlu-marketing',
                'cmmlu-marxist_theory',
                'cmmlu-modern_chinese',
                'cmmlu-nutrition',
                'cmmlu-philosophy',
                'cmmlu-professional_accounting',
                'cmmlu-professional_law',
                'cmmlu-professional_medicine',
                'cmmlu-professional_psychology',
                'cmmlu-public_relations',
                'cmmlu-security_study',
                'cmmlu-sociology',
                'cmmlu-sports_science',
                'cmmlu-traditional_chinese_medicine',
                'cmmlu-virology',
                'cmmlu-world_history',
                'cmmlu-world_religions',
                ]),
        dict(name='ceval-stem',
            subsets=[
                'ceval-computer_network',
                'ceval-operating_system',
                'ceval-computer_architecture',
                'ceval-college_programming',
                'ceval-college_physics',
                'ceval-college_chemistry',
                'ceval-advanced_mathematics',
                'ceval-probability_and_statistics',
                'ceval-discrete_mathematics',
                'ceval-electrical_engineer',
                'ceval-metrology_engineer',
                'ceval-high_school_mathematics',
                'ceval-high_school_physics',
                'ceval-high_school_chemistry',
                'ceval-high_school_biology',
                'ceval-middle_school_mathematics',
                'ceval-middle_school_biology',
                'ceval-middle_school_physics',
                'ceval-middle_school_chemistry',
                'ceval-veterinary_medicine',
                ]),
        dict(name='ceval-social-science',
            subsets=[
                'ceval-college_economics',
                'ceval-business_administration',
                'ceval-marxism',
                'ceval-mao_zedong_thought',
                'ceval-education_science',
                'ceval-teacher_qualification',
                'ceval-high_school_politics',
                'ceval-high_school_geography',
                'ceval-middle_school_politics',
                'ceval-middle_school_geography',
                ]),
        dict(name='ceval-humanities',
            subsets=[
                'ceval-modern_chinese_history',
                'ceval-ideological_and_moral_cultivation',
                'ceval-logic',
                'ceval-law',
                'ceval-chinese_language_and_literature',
                'ceval-art_studies',
                'ceval-professional_tour_guide',
                'ceval-legal_professional',
                'ceval-high_school_chinese',
                'ceval-high_school_history',
                'ceval-middle_school_history',
                ]),
        dict(name='ceval-other',
            subsets=[
                'ceval-civil_servant',
                'ceval-sports_science',
                'ceval-plant_protection',
                'ceval-basic_medicine',
                'ceval-clinical_medicine',
                'ceval-urban_and_rural_planner',
                'ceval-accountant',
                'ceval-fire_engineer',
                'ceval-environmental_impact_assessment_engineer',
                'ceval-tax_accountant',
                'ceval-physician',
                ]),
        dict(name='ceval-hard',
            subsets=[
                'ceval-advanced_mathematics',
                'ceval-discrete_mathematics',
                'ceval-probability_and_statistics',
                'ceval-college_chemistry',
                'ceval-college_physics',
                'ceval-high_school_mathematics',
                'ceval-high_school_chemistry',
                'ceval-high_school_physics',
                ]),
        dict(name='ceval',
            subsets=[
                'ceval-computer_network',
                'ceval-operating_system',
                'ceval-computer_architecture',
                'ceval-college_programming',
                'ceval-college_physics',
                'ceval-college_chemistry',
                'ceval-advanced_mathematics',
                'ceval-probability_and_statistics',
                'ceval-discrete_mathematics',
                'ceval-electrical_engineer',
                'ceval-metrology_engineer',
                'ceval-high_school_mathematics',
                'ceval-high_school_physics',
                'ceval-high_school_chemistry',
                'ceval-high_school_biology',
                'ceval-middle_school_mathematics',
                'ceval-middle_school_biology',
                'ceval-middle_school_physics',
                'ceval-middle_school_chemistry',
                'ceval-veterinary_medicine',
                'ceval-college_economics',
                'ceval-business_administration',
                'ceval-marxism',
                'ceval-mao_zedong_thought',
                'ceval-education_science',
                'ceval-teacher_qualification',
                'ceval-high_school_politics',
                'ceval-high_school_geography',
                'ceval-middle_school_politics',
                'ceval-middle_school_geography',
                'ceval-modern_chinese_history',
                'ceval-ideological_and_moral_cultivation',
                'ceval-logic',
                'ceval-law',
                'ceval-chinese_language_and_literature',
                'ceval-art_studies',
                'ceval-professional_tour_guide',
                'ceval-legal_professional',
                'ceval-high_school_chinese',
                'ceval-high_school_history',
                'ceval-middle_school_history',
                'ceval-civil_servant',
                'ceval-sports_science',
                'ceval-plant_protection',
                'ceval-basic_medicine',
                'ceval-clinical_medicine',
                'ceval-urban_and_rural_planner',
                'ceval-accountant',
                'ceval-fire_engineer',
                'ceval-environmental_impact_assessment_engineer',
                'ceval-tax_accountant',
                'ceval-physician',
                ]),
        dict(name='ceval-test-stem',
            subsets=[
                'ceval-test-computer_network',
                'ceval-test-operating_system',
                'ceval-test-computer_architecture',
                'ceval-test-college_programming',
                'ceval-test-college_physics',
                'ceval-test-college_chemistry',
                'ceval-test-advanced_mathematics',
                'ceval-test-probability_and_statistics',
                'ceval-test-discrete_mathematics',
                'ceval-test-electrical_engineer',
                'ceval-test-metrology_engineer',
                'ceval-test-high_school_mathematics',
                'ceval-test-high_school_physics',
                'ceval-test-high_school_chemistry',
                'ceval-test-high_school_biology',
                'ceval-test-middle_school_mathematics',
                'ceval-test-middle_school_biology',
                'ceval-test-middle_school_physics',
                'ceval-test-middle_school_chemistry',
                'ceval-test-veterinary_medicine',
                ]),
        dict(name='ceval-test-social-science',
            subsets=[
                'ceval-test-college_economics',
                'ceval-test-business_administration',
                'ceval-test-marxism',
                'ceval-test-mao_zedong_thought',
                'ceval-test-education_science',
                'ceval-test-teacher_qualification',
                'ceval-test-high_school_politics',
                'ceval-test-high_school_geography',
                'ceval-test-middle_school_politics',
                'ceval-test-middle_school_geography',
                ]),
        dict(name='ceval-test-humanities',
            subsets=[
                'ceval-test-modern_chinese_history',
                'ceval-test-ideological_and_moral_cultivation',
                'ceval-test-logic',
                'ceval-test-law',
                'ceval-test-chinese_language_and_literature',
                'ceval-test-art_studies',
                'ceval-test-professional_tour_guide',
                'ceval-test-legal_professional',
                'ceval-test-high_school_chinese',
                'ceval-test-high_school_history',
                'ceval-test-middle_school_history',
                ]),
        dict(name='ceval-test-other',
            subsets=[
                'ceval-test-civil_servant',
                'ceval-test-sports_science',
                'ceval-test-plant_protection',
                'ceval-test-basic_medicine',
                'ceval-test-clinical_medicine',
                'ceval-test-urban_and_rural_planner',
                'ceval-test-accountant',
                'ceval-test-fire_engineer',
                'ceval-test-environmental_impact_assessment_engineer',
                'ceval-test-tax_accountant',
                'ceval-test-physician',
                ]),
        dict(name='ceval-test-hard',
            subsets=[
                'ceval-test-advanced_mathematics',
                'ceval-test-discrete_mathematics',
                'ceval-test-probability_and_statistics',
                'ceval-test-college_chemistry',
                'ceval-test-college_physics',
                'ceval-test-high_school_mathematics',
                'ceval-test-high_school_chemistry',
                'ceval-test-high_school_physics',
                ]),
        dict(name='ceval-test',
            subsets=[
                'ceval-test-computer_network',
                'ceval-test-operating_system',
                'ceval-test-computer_architecture',
                'ceval-test-college_programming',
                'ceval-test-college_physics',
                'ceval-test-college_chemistry',
                'ceval-test-advanced_mathematics',
                'ceval-test-probability_and_statistics',
                'ceval-test-discrete_mathematics',
                'ceval-test-electrical_engineer',
                'ceval-test-metrology_engineer',
                'ceval-test-high_school_mathematics',
                'ceval-test-high_school_physics',
                'ceval-test-high_school_chemistry',
                'ceval-test-high_school_biology',
                'ceval-test-middle_school_mathematics',
                'ceval-test-middle_school_biology',
                'ceval-test-middle_school_physics',
                'ceval-test-middle_school_chemistry',
                'ceval-test-veterinary_medicine',
                'ceval-test-college_economics',
                'ceval-test-business_administration',
                'ceval-test-marxism',
                'ceval-test-mao_zedong_thought',
                'ceval-test-education_science',
                'ceval-test-teacher_qualification',
                'ceval-test-high_school_politics',
                'ceval-test-high_school_geography',
                'ceval-test-middle_school_politics',
                'ceval-test-middle_school_geography',
                'ceval-test-modern_chinese_history',
                'ceval-test-ideological_and_moral_cultivation',
                'ceval-test-logic',
                'ceval-test-law',
                'ceval-test-chinese_language_and_literature',
                'ceval-test-art_studies',
                'ceval-test-professional_tour_guide',
                'ceval-test-legal_professional',
                'ceval-test-high_school_chinese',
                'ceval-test-high_school_history',
                'ceval-test-middle_school_history',
                'ceval-test-civil_servant',
                'ceval-test-sports_science',
                'ceval-test-plant_protection',
                'ceval-test-basic_medicine',
                'ceval-test-clinical_medicine',
                'ceval-test-urban_and_rural_planner',
                'ceval-test-accountant',
                'ceval-test-fire_engineer',
                'ceval-test-environmental_impact_assessment_engineer',
                'ceval-test-tax_accountant',
                'ceval-test-physician',
                ]),
        dict(name='bbh',
            subsets=[
                'bbh-temporal_sequences',
                'bbh-disambiguation_qa',
                'bbh-date_understanding',
                'bbh-tracking_shuffled_objects_three_objects',
                'bbh-penguins_in_a_table',
                'bbh-geometric_shapes',
                'bbh-snarks',
                'bbh-ruin_names',
                'bbh-tracking_shuffled_objects_seven_objects',
                'bbh-tracking_shuffled_objects_five_objects',
                'bbh-logical_deduction_three_objects',
                'bbh-hyperbaton',
                'bbh-logical_deduction_five_objects',
                'bbh-logical_deduction_seven_objects',
                'bbh-movie_recommendation',
                'bbh-salient_translation_error_detection',
                'bbh-reasoning_about_colored_objects',
                'bbh-multistep_arithmetic_two',
                'bbh-navigate',
                'bbh-dyck_languages',
                'bbh-word_sorting',
                'bbh-sports_understanding',
                'bbh-boolean_expressions',
                'bbh-object_counting',
                'bbh-formal_fallacies',
                'bbh-causal_judgement',
                'bbh-web_of_lies',
                ]),
        dict(name='GaokaoBench',
            subsets=[
                'GaokaoBench_2010-2022_Math_II_MCQs',
                'GaokaoBench_2010-2022_Math_I_MCQs',
                'GaokaoBench_2010-2022_History_MCQs',
                'GaokaoBench_2010-2022_Biology_MCQs',
                'GaokaoBench_2010-2022_Political_Science_MCQs',
                'GaokaoBench_2010-2022_Physics_MCQs',
                'GaokaoBench_2010-2022_Chemistry_MCQs',
                'GaokaoBench_2010-2013_English_MCQs',
                'GaokaoBench_2010-2022_Chinese_Modern_Lit',
                'GaokaoBench_2010-2022_English_Fill_in_Blanks',
                'GaokaoBench_2012-2022_English_Cloze_Test',
                'GaokaoBench_2010-2022_Geography_MCQs',
                'GaokaoBench_2010-2022_English_Reading_Comp',
                'GaokaoBench_2010-2022_Chinese_Lang_and_Usage_MCQs',
                ],
            weights=dict(
                {'GaokaoBench_2010-2013_English_MCQs': 105,
                'GaokaoBench_2010-2022_Biology_MCQs': 900,
                'GaokaoBench_2010-2022_Chemistry_MCQs': 744,
                'GaokaoBench_2010-2022_Chinese_Lang_and_Usage_MCQs': 240,
                'GaokaoBench_2010-2022_Chinese_Modern_Lit': 261,
                'GaokaoBench_2010-2022_English_Fill_in_Blanks': 900.0,
                'GaokaoBench_2010-2022_English_Reading_Comp': 940,
                'GaokaoBench_2010-2022_Geography_MCQs': 380,
                'GaokaoBench_2010-2022_History_MCQs': 1148,
                'GaokaoBench_2010-2022_Math_II_MCQs': 1090,
                'GaokaoBench_2010-2022_Math_I_MCQs': 1070,
                'GaokaoBench_2010-2022_Physics_MCQs': 384,
                'GaokaoBench_2010-2022_Political_Science_MCQs': 1280,
                'GaokaoBench_2012-2022_English_Cloze_Test': 260})),
        dict(name='flores_100_Indo-European-Germanic_English',
            subsets=[
                'flores_100_afr-eng',
                'flores_100_dan-eng',
                'flores_100_deu-eng',
                'flores_100_isl-eng',
                'flores_100_ltz-eng',
                'flores_100_nld-eng',
                'flores_100_nob-eng',
                'flores_100_swe-eng',
                ]),
        dict(name='flores_100_English_Indo-European-Germanic',
            subsets=[
                'flores_100_eng-afr',
                'flores_100_eng-dan',
                'flores_100_eng-deu',
                'flores_100_eng-isl',
                'flores_100_eng-ltz',
                'flores_100_eng-nld',
                'flores_100_eng-nob',
                'flores_100_eng-swe',
                ]),
        dict(name='flores_100_Indo-European-Romance_English',
            subsets=[
                'flores_100_ast-eng',
                'flores_100_cat-eng',
                'flores_100_fra-eng',
                'flores_100_glg-eng',
                'flores_100_oci-eng',
                'flores_100_por-eng',
                'flores_100_ron-eng',
                'flores_100_spa-eng',
                ]),
        dict(name='flores_100_English_Indo-European-Romance',
            subsets=[
                'flores_100_eng-ast',
                'flores_100_eng-cat',
                'flores_100_eng-fra',
                'flores_100_eng-glg',
                'flores_100_eng-oci',
                'flores_100_eng-por',
                'flores_100_eng-ron',
                'flores_100_eng-spa',
                ]),
        dict(name='flores_100_Indo-European-Slavic_English',
            subsets=[
                'flores_100_bel-eng',
                'flores_100_bos-eng',
                'flores_100_bul-eng',
                'flores_100_ces-eng',
                'flores_100_hrv-eng',
                'flores_100_mkd-eng',
                'flores_100_pol-eng',
                'flores_100_rus-eng',
                'flores_100_slk-eng',
                'flores_100_slv-eng',
                'flores_100_srp-eng',
                'flores_100_ukr-eng',
                ]),
        dict(name='flores_100_English_Indo-European-Slavic',
            subsets=[
                'flores_100_eng-bel',
                'flores_100_eng-bos',
                'flores_100_eng-bul',
                'flores_100_eng-ces',
                'flores_100_eng-hrv',
                'flores_100_eng-mkd',
                'flores_100_eng-pol',
                'flores_100_eng-rus',
                'flores_100_eng-slk',
                'flores_100_eng-slv',
                'flores_100_eng-srp',
                'flores_100_eng-ukr',
                ]),
        dict(name='flores_100_Indo-European-Indo-Aryan_English',
            subsets=[
                'flores_100_asm-eng',
                'flores_100_ben-eng',
                'flores_100_guj-eng',
                'flores_100_hin-eng',
                'flores_100_mar-eng',
                'flores_100_npi-eng',
                'flores_100_ory-eng',
                'flores_100_pan-eng',
                'flores_100_snd-eng',
                'flores_100_urd-eng',
                ]),
        dict(name='flores_100_English_Indo-European-Indo-Aryan',
            subsets=[
                'flores_100_eng-asm',
                'flores_100_eng-ben',
                'flores_100_eng-guj',
                'flores_100_eng-hin',
                'flores_100_eng-mar',
                'flores_100_eng-npi',
                'flores_100_eng-ory',
                'flores_100_eng-pan',
                'flores_100_eng-snd',
                'flores_100_eng-urd',
                ]),
        dict(name='flores_100_Indo-European-Other_English',
            subsets=[
                'flores_100_ckb-eng',
                'flores_100_cym-eng',
                'flores_100_ell-eng',
                'flores_100_fas-eng',
                'flores_100_gle-eng',
                'flores_100_hye-eng',
                'flores_100_ita-eng',
                'flores_100_lav-eng',
                'flores_100_lit-eng',
                'flores_100_pus-eng',
                'flores_100_tgk-eng',
                ]),
        dict(name='flores_100_English_Indo-European-Other',
            subsets=[
                'flores_100_eng-ckb',
                'flores_100_eng-cym',
                'flores_100_eng-ell',
                'flores_100_eng-fas',
                'flores_100_eng-gle',
                'flores_100_eng-hye',
                'flores_100_eng-ita',
                'flores_100_eng-lav',
                'flores_100_eng-lit',
                'flores_100_eng-pus',
                'flores_100_eng-tgk',
                ]),
        dict(name='flores_100_Austronesian_English',
            subsets=[
                'flores_100_ceb-eng',
                'flores_100_ind-eng',
                'flores_100_jav-eng',
                'flores_100_mri-eng',
                'flores_100_msa-eng',
                'flores_100_tgl-eng',
                ]),
        dict(name='flores_100_English_Austronesian',
            subsets=[
                'flores_100_eng-ceb',
                'flores_100_eng-ind',
                'flores_100_eng-jav',
                'flores_100_eng-mri',
                'flores_100_eng-msa',
                'flores_100_eng-tgl',
                ]),
        dict(name='flores_100_Atlantic-Congo_English',
            subsets=[
                'flores_100_ibo-eng',
                'flores_100_kam-eng',
                'flores_100_kea-eng',
                'flores_100_lin-eng',
                'flores_100_lug-eng',
                'flores_100_nso-eng',
                'flores_100_nya-eng',
                'flores_100_sna-eng',
                'flores_100_swh-eng',
                'flores_100_umb-eng',
                'flores_100_wol-eng',
                'flores_100_xho-eng',
                'flores_100_yor-eng',
                'flores_100_zul-eng',
                ]),
        dict(name='flores_100_English_Atlantic-Congo',
            subsets=[
                'flores_100_eng-ibo',
                'flores_100_eng-kam',
                'flores_100_eng-kea',
                'flores_100_eng-lin',
                'flores_100_eng-lug',
                'flores_100_eng-nso',
                'flores_100_eng-nya',
                'flores_100_eng-sna',
                'flores_100_eng-swh',
                'flores_100_eng-umb',
                'flores_100_eng-wol',
                'flores_100_eng-xho',
                'flores_100_eng-yor',
                'flores_100_eng-zul',
                ]),
        dict(name='flores_100_Afro-Asiatic_English',
            subsets=[
                'flores_100_amh-eng',
                'flores_100_ara-eng',
                'flores_100_ful-eng',
                'flores_100_mlt-eng',
                'flores_100_orm-eng',
                'flores_100_som-eng',
                ]),
        dict(name='flores_100_English_Afro-Asiatic',
            subsets=[
                'flores_100_eng-amh',
                'flores_100_eng-ara',
                'flores_100_eng-ful',
                'flores_100_eng-mlt',
                'flores_100_eng-orm',
                'flores_100_eng-som',
                ]),
        dict(name='flores_100_Turkic_English',
            subsets=[
                'flores_100_azj-eng',
                'flores_100_kaz-eng',
                'flores_100_kir-eng',
                'flores_100_tur-eng',
                'flores_100_uzb-eng',
                ]),
        dict(name='flores_100_English_Turkic',
            subsets=[
                'flores_100_eng-azj',
                'flores_100_eng-kaz',
                'flores_100_eng-kir',
                'flores_100_eng-tur',
                'flores_100_eng-uzb',
                ]),
        dict(name='flores_100_Dravidian_English',
            subsets=[
                'flores_100_kan-eng',
                'flores_100_mal-eng',
                'flores_100_tam-eng',
                'flores_100_tel-eng',
                ]),
        dict(name='flores_100_English_Dravidian',
            subsets=[
                'flores_100_eng-kan',
                'flores_100_eng-mal',
                'flores_100_eng-tam',
                'flores_100_eng-tel',
                ]),
        dict(name='flores_100_Sino-Tibetan_English',
            subsets=[
                'flores_100_mya-eng',
                'flores_100_zho_simpl-eng',
                'flores_100_zho_trad-eng',
                ]),
        dict(name='flores_100_English_Sino-Tibetan',
            subsets=[
                'flores_100_eng-mya',
                'flores_100_eng-zho_simpl',
                'flores_100_eng-zho_trad',
                ]),
        dict(name='flores_100_Other_English',
            subsets=[
                'flores_100_est-eng',
                'flores_100_fin-eng',
                'flores_100_hau-eng',
                'flores_100_heb-eng',
                'flores_100_hun-eng',
                'flores_100_jpn-eng',
                'flores_100_kat-eng',
                'flores_100_khm-eng',
                'flores_100_kor-eng',
                'flores_100_lao-eng',
                'flores_100_luo-eng',
                'flores_100_mon-eng',
                'flores_100_tha-eng',
                'flores_100_vie-eng',
                ]),
        dict(name='flores_100_English_Other',
            subsets=[
                'flores_100_eng-est',
                'flores_100_eng-fin',
                'flores_100_eng-hau',
                'flores_100_eng-heb',
                'flores_100_eng-hun',
                'flores_100_eng-jpn',
                'flores_100_eng-kat',
                'flores_100_eng-khm',
                'flores_100_eng-kor',
                'flores_100_eng-lao',
                'flores_100_eng-luo',
                'flores_100_eng-mon',
                'flores_100_eng-tha',
                'flores_100_eng-vie',
                ]),
        dict(name='flores_100',
            subsets=[
                'flores_100_afr-eng',
                'flores_100_dan-eng',
                'flores_100_deu-eng',
                'flores_100_isl-eng',
                'flores_100_ltz-eng',
                'flores_100_nld-eng',
                'flores_100_nob-eng',
                'flores_100_swe-eng',
                'flores_100_ast-eng',
                'flores_100_cat-eng',
                'flores_100_fra-eng',
                'flores_100_glg-eng',
                'flores_100_oci-eng',
                'flores_100_por-eng',
                'flores_100_ron-eng',
                'flores_100_spa-eng',
                'flores_100_bel-eng',
                'flores_100_bos-eng',
                'flores_100_bul-eng',
                'flores_100_ces-eng',
                'flores_100_hrv-eng',
                'flores_100_mkd-eng',
                'flores_100_pol-eng',
                'flores_100_rus-eng',
                'flores_100_slk-eng',
                'flores_100_slv-eng',
                'flores_100_srp-eng',
                'flores_100_ukr-eng',
                'flores_100_asm-eng',
                'flores_100_ben-eng',
                'flores_100_guj-eng',
                'flores_100_hin-eng',
                'flores_100_mar-eng',
                'flores_100_npi-eng',
                'flores_100_ory-eng',
                'flores_100_pan-eng',
                'flores_100_snd-eng',
                'flores_100_urd-eng',
                'flores_100_ckb-eng',
                'flores_100_cym-eng',
                'flores_100_ell-eng',
                'flores_100_fas-eng',
                'flores_100_gle-eng',
                'flores_100_hye-eng',
                'flores_100_ita-eng',
                'flores_100_lav-eng',
                'flores_100_lit-eng',
                'flores_100_pus-eng',
                'flores_100_tgk-eng',
                'flores_100_ceb-eng',
                'flores_100_ind-eng',
                'flores_100_jav-eng',
                'flores_100_mri-eng',
                'flores_100_msa-eng',
                'flores_100_tgl-eng',
                'flores_100_ibo-eng',
                'flores_100_kam-eng',
                'flores_100_kea-eng',
                'flores_100_lin-eng',
                'flores_100_lug-eng',
                'flores_100_nso-eng',
                'flores_100_nya-eng',
                'flores_100_sna-eng',
                'flores_100_swh-eng',
                'flores_100_umb-eng',
                'flores_100_wol-eng',
                'flores_100_xho-eng',
                'flores_100_yor-eng',
                'flores_100_zul-eng',
                'flores_100_amh-eng',
                'flores_100_ara-eng',
                'flores_100_ful-eng',
                'flores_100_mlt-eng',
                'flores_100_orm-eng',
                'flores_100_som-eng',
                'flores_100_azj-eng',
                'flores_100_kaz-eng',
                'flores_100_kir-eng',
                'flores_100_tur-eng',
                'flores_100_uzb-eng',
                'flores_100_kan-eng',
                'flores_100_mal-eng',
                'flores_100_tam-eng',
                'flores_100_tel-eng',
                'flores_100_mya-eng',
                'flores_100_zho_simpl-eng',
                'flores_100_zho_trad-eng',
                'flores_100_est-eng',
                'flores_100_fin-eng',
                'flores_100_hau-eng',
                'flores_100_heb-eng',
                'flores_100_hun-eng',
                'flores_100_jpn-eng',
                'flores_100_kat-eng',
                'flores_100_khm-eng',
                'flores_100_kor-eng',
                'flores_100_lao-eng',
                'flores_100_luo-eng',
                'flores_100_mon-eng',
                'flores_100_tha-eng',
                'flores_100_vie-eng',
                'flores_100_eng-afr',
                'flores_100_eng-dan',
                'flores_100_eng-deu',
                'flores_100_eng-isl',
                'flores_100_eng-ltz',
                'flores_100_eng-nld',
                'flores_100_eng-nob',
                'flores_100_eng-swe',
                'flores_100_eng-ast',
                'flores_100_eng-cat',
                'flores_100_eng-fra',
                'flores_100_eng-glg',
                'flores_100_eng-oci',
                'flores_100_eng-por',
                'flores_100_eng-ron',
                'flores_100_eng-spa',
                'flores_100_eng-bel',
                'flores_100_eng-bos',
                'flores_100_eng-bul',
                'flores_100_eng-ces',
                'flores_100_eng-hrv',
                'flores_100_eng-mkd',
                'flores_100_eng-pol',
                'flores_100_eng-rus',
                'flores_100_eng-slk',
                'flores_100_eng-slv',
                'flores_100_eng-srp',
                'flores_100_eng-ukr',
                'flores_100_eng-asm',
                'flores_100_eng-ben',
                'flores_100_eng-guj',
                'flores_100_eng-hin',
                'flores_100_eng-mar',
                'flores_100_eng-npi',
                'flores_100_eng-ory',
                'flores_100_eng-pan',
                'flores_100_eng-snd',
                'flores_100_eng-urd',
                'flores_100_eng-ckb',
                'flores_100_eng-cym',
                'flores_100_eng-ell',
                'flores_100_eng-fas',
                'flores_100_eng-gle',
                'flores_100_eng-hye',
                'flores_100_eng-ita',
                'flores_100_eng-lav',
                'flores_100_eng-lit',
                'flores_100_eng-pus',
                'flores_100_eng-tgk',
                'flores_100_eng-ceb',
                'flores_100_eng-ind',
                'flores_100_eng-jav',
                'flores_100_eng-mri',
                'flores_100_eng-msa',
                'flores_100_eng-tgl',
                'flores_100_eng-ibo',
                'flores_100_eng-kam',
                'flores_100_eng-kea',
                'flores_100_eng-lin',
                'flores_100_eng-lug',
                'flores_100_eng-nso',
                'flores_100_eng-nya',
                'flores_100_eng-sna',
                'flores_100_eng-swh',
                'flores_100_eng-umb',
                'flores_100_eng-wol',
                'flores_100_eng-xho',
                'flores_100_eng-yor',
                'flores_100_eng-zul',
                'flores_100_eng-amh',
                'flores_100_eng-ara',
                'flores_100_eng-ful',
                'flores_100_eng-mlt',
                'flores_100_eng-orm',
                'flores_100_eng-som',
                'flores_100_eng-azj',
                'flores_100_eng-kaz',
                'flores_100_eng-kir',
                'flores_100_eng-tur',
                'flores_100_eng-uzb',
                'flores_100_eng-kan',
                'flores_100_eng-mal',
                'flores_100_eng-tam',
                'flores_100_eng-tel',
                'flores_100_eng-mya',
                'flores_100_eng-zho_simpl',
                'flores_100_eng-zho_trad',
                'flores_100_eng-est',
                'flores_100_eng-fin',
                'flores_100_eng-hau',
                'flores_100_eng-heb',
                'flores_100_eng-hun',
                'flores_100_eng-jpn',
                'flores_100_eng-kat',
                'flores_100_eng-khm',
                'flores_100_eng-kor',
                'flores_100_eng-lao',
                'flores_100_eng-luo',
                'flores_100_eng-mon',
                'flores_100_eng-tha',
                'flores_100_eng-vie',
                ]),
        dict(name='jigsaw_multilingual',
            subsets=[
                'jigsaw_multilingual_es',
                'jigsaw_multilingual_fr',
                'jigsaw_multilingual_it',
                'jigsaw_multilingual_pt',
                'jigsaw_multilingual_ru',
                'jigsaw_multilingual_tr',
                ]),
        dict(name='tydiqa-goldp',
            subsets=[
                'tydiqa-goldp_arabic',
                'tydiqa-goldp_bengali',
                'tydiqa-goldp_english',
                'tydiqa-goldp_finnish',
                'tydiqa-goldp_indonesian',
                'tydiqa-goldp_japanese',
                'tydiqa-goldp_korean',
                'tydiqa-goldp_russian',
                'tydiqa-goldp_swahili',
                'tydiqa-goldp_telugu',
                'tydiqa-goldp_thai',
                ]),
        dict(name='xiezhi',
            subsets=[
                'xiezhi-spec_eng',
                'xiezhi-spec_chn',
                'xiezhi-inter_eng',
                'xiezhi-inter_chn',
                ]),
        ])
work_dir='outputs/api_qwen/20240516_162520'